{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pickle as pkl\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Twitter API credentials\n",
    "bearer_token = <YOUR BEARER TOKEN>\n",
    "\n",
    "# Create API client\n",
    "client = tweepy.Client(bearer_token)\n",
    "\n",
    "# # --- Database connection details ---\n",
    "DB_HOST = <YOUR DB HOST>\n",
    "DB_USER = <YOUR DB USER>\n",
    "DB_PASSWORD = <YOUR DB PASSWORD>\n",
    "DB_NAME = <YOUR DB NAME>\n",
    "\n",
    "# --- Connect to the database ---\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_NAME,\n",
    "    charset=\"utf8mb4\",\n",
    "    collation=\"utf8mb4_unicode_ci\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = db_connection.cursor()"
   ],
   "id": "dad33025173e4ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load tweet ids\n",
    "with open('top_tweetIds.pkl', 'rb') as f:\n",
    "    tweet_ids = pkl.load(f)\n",
    "tweet_ids = [str(id) for id in tweet_ids]\n",
    "print(len(tweet_ids))"
   ],
   "id": "d6ddba4d9d477f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to fetch tweets by IDs\n",
    "def fetch_tweets(ids):\n",
    "    tweets_response = client.get_tweets(ids,\n",
    "                                        tweet_fields=['author_id','created_at','public_metrics','source'],\n",
    "                                        user_fields=['affiliation','public_metrics','username','verified','verified_type','parody'])\n",
    "    data = list(tweets_response.data)\n",
    "    tweets_data = [dict(tweet_data) for tweet_data in data]\n",
    "    return tweets_data\n",
    "\n",
    "# Function to flatten tweet data\n",
    "def flatten_tweet_data(data):\n",
    "    flat_data = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            flat_data.update(flatten_tweet_data(value))\n",
    "        else:\n",
    "            flat_data[key] = value\n",
    "    return flat_data\n",
    "\n",
    "# Function to insert tweets into the database\n",
    "def store_tweets(tweets_data):\n",
    "    insert_query = \"\"\"\n",
    "INSERT IGNORE INTO tweets (\n",
    "    created_at,\n",
    "    retweet_count,\n",
    "    reply_count,\n",
    "    like_count,\n",
    "    quote_count,\n",
    "    bookmark_count,\n",
    "    impression_count,\n",
    "    text,\n",
    "    edit_history_tweet_ids,\n",
    "    author_id,\n",
    "    id\n",
    ")\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "    # Insert each tweet into the database\n",
    "    for tweet in tweets_data:\n",
    "        edit_history_json = json.dumps(tweet[\"edit_history_tweet_ids\"])\n",
    "        values = (\n",
    "            tweet['created_at'],\n",
    "            tweet['retweet_count'],\n",
    "            tweet['reply_count'],\n",
    "            tweet['like_count'],\n",
    "            tweet['quote_count'],\n",
    "            tweet['bookmark_count'],\n",
    "            tweet['impression_count'],\n",
    "            tweet['text'],\n",
    "            edit_history_json,\n",
    "            tweet['author_id'],\n",
    "            tweet['id']\n",
    "        )\n",
    "    \n",
    "        cursor.execute(insert_query, values)\n",
    "    \n",
    "    # Commit the transaction\n",
    "    db_connection.commit()\n",
    "    \n",
    "# Function to fetch and store tweets\n",
    "def fetch_and_store(ids):\n",
    "    tweets_data = fetch_tweets(ids)\n",
    "    flat_tweets_data = [flatten_tweet_data(tweet) for tweet in tweets_data]\n",
    "    # convert datetime to string\n",
    "    for tweet_data in flat_tweets_data:\n",
    "        tweet_data['created_at'] = tweet_data['created_at'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    store_tweets(flat_tweets_data)"
   ],
   "id": "c2e78f40f80557bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fetch and store tweets in batches\n",
    "batch_size = 100\n",
    "start_at = 1450\n",
    "for i in tqdm(range(start_at, len(tweet_ids), batch_size), desc=\"Processing tweets\", unit=\"batch\", unit_scale=True):\n",
    "    end_i = i + batch_size if i + batch_size < len(tweet_ids) else len(tweet_ids)\n",
    "    print(f\"Processing tweets {i} to {end_i}\")\n",
    "    try:\n",
    "        fetch_and_store(tweet_ids[i:end_i])\n",
    "        # sleep for 1 minute to avoid rate limit\n",
    "        time.sleep(70)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweets {i} to {end_i}: {e}\")\n",
    "        "
   ],
   "id": "1f0a274f4267f6e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Clean up ---\n",
    "cursor.close()\n",
    "db_connection.close()"
   ],
   "id": "dcc0fdde44408f2d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
